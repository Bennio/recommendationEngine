{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Recommender System with SageMaker, MXNet, and Gluon\n",
    "_**Making Video Recommendations Using Neural Networks and Embeddings**_\n",
    "\n",
    "--- \n",
    "\n",
    "---\n",
    "\n",
    "*This work is based on content from the [Cyrus Vahid's 2017 re:Invent Talk](https://github.com/cyrusmvahid/gluontutorials/blob/master/recommendations/MLPMF.ipynb)*\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "  1. [Explore](#Explore)\n",
    "  1. [Clean](#Clean)\n",
    "  1. [Prepare](#Prepare)\n",
    "1. [Train Locally](#Train-Locally)\n",
    "  1. [Define Network](#Define-Network)\n",
    "  1. [Set Parameters](#Set-Parameters)\n",
    "  1. [Execute](#Execute)\n",
    "1. [Train with SageMaker](#Train-with-SageMaker)\n",
    "  1. [Wrap Code](#Wrap-Code)\n",
    "  1. [Move Data](#Move-Data)\n",
    "  1. [Submit](#Submit)\n",
    "1. [Host](#Host)\n",
    "  1. [Evaluate](#Evaluate)\n",
    "1. [Wrap-up](#Wrap-up)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "In many ways, recommender systems were a catalyst for the current popularity of machine learning.  One of Amazon's earliest successes was the \"Customers who bought this, also bought...\" feature, while the million dollar Netflix Prize spurred research, raised public awareness, and inspired numerous other data science competitions.\n",
    "\n",
    "Recommender systems can utilize a multitude of data sources and ML algorithms, and most combine various unsupervised, supervised, and reinforcement learning techniques into a holistic framework.  However, the core component is almost always a model which which predicts a user's rating (or purchase) for a certain item based on that user's historical ratings of similar items as well as the behavior of other similar users.  The minimal required dataset for this is a history of user item ratings.  In our case, we'll use 1 to 5 star ratings from over 2M Amazon customers on over 160K digital videos.  More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "Matrix factorization has been the cornerstone of most user-item prediction models.  This method starts with the large, sparse, user-item ratings in a single matrix, where users index the rows, and items index the columns.  It then seeks to find two lower-dimensional, dense matrices which, when multiplied together, preserve the information and relationships in the larger matrix.\n",
    "\n",
    "Matrix factorization has been extended and genarlized with deep learning and embeddings.  These techniques allows us to introduce non-linearities for enhanced performance and flexibility.  This notebook will fit a neural network-based model to generate recommendations for the Amazon video dataset.  It will start by exploring our data in the notebook and even training a model on a sample of the data.  Later we'll expand to the full dataset and fit our model using a SageMaker managed training cluster.  We'll then deploy to an endpoint and check our method.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p2.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `get_execution_role()` call with the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "bucket = 'layup232345'\n",
    "prefix = 'sagemaker/DEMO-gluon-recsys'\n",
    "\n",
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the Python libraries we'll need for the remainder of this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, ndarray\n",
    "from mxnet.metric import MSE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "import boto3\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "### Explore\n",
    "\n",
    "Let's start by bringing in our dataset from an S3 public bucket.  As mentioned above, this contains 1 to 5 star ratings from over 2M Amazon customers on over 160K digital videos.  More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "_Note, because this dataset is over a half gigabyte, the load from S3 may take ~10 minutes.  Also, since Amazon SageMaker Notebooks start with a 5GB persistent volume by default, and we don't need to keep this data on our instance for long, we'll bring it to the temporary volume (which has up to 20GB of storage)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/tmp/recsys/’: File exists\n",
      "download: s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz to ../../../../tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "# !mkdir /tmp/recsys/\n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz /tmp/recsys/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) so that we can begin to understand it.\n",
    "\n",
    "*Note, we'll set `error_bad_lines=False` when reading the file in as there appear to be a very small number of records which would create a problem otherwise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 92523: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 343254: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 524626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 623024: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 977412: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1496867: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1711638: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1787213: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2395306: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2527690: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>12190288</td>\n",
       "      <td>R3FU16928EP5TC</td>\n",
       "      <td>B00AYB1482</td>\n",
       "      <td>668895143</td>\n",
       "      <td>Enlightened: Season 1</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>I loved it and I wish there was a season 3</td>\n",
       "      <td>I loved it and I wish there was a season 3... ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>30549954</td>\n",
       "      <td>R1IZHHS1MH3AQ4</td>\n",
       "      <td>B00KQD28OM</td>\n",
       "      <td>246219280</td>\n",
       "      <td>Vicious</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>52895410</td>\n",
       "      <td>R52R85WC6TIAH</td>\n",
       "      <td>B01489L5LQ</td>\n",
       "      <td>534732318</td>\n",
       "      <td>After Words</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Charming movie</td>\n",
       "      <td>This movie isn't perfect, but it gets a lot of...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>27072354</td>\n",
       "      <td>R7HOOYTVIB0DS</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>239012694</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>excellant this is what tv should be</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26939022</td>\n",
       "      <td>R1XQ2N5CDOZGNX</td>\n",
       "      <td>B0094LZMT0</td>\n",
       "      <td>535858974</td>\n",
       "      <td>On The Waterfront</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Brilliant film from beginning to end</td>\n",
       "      <td>Brilliant film from beginning to end. All of t...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     12190288  R3FU16928EP5TC  B00AYB1482       668895143   \n",
       "1          US     30549954  R1IZHHS1MH3AQ4  B00KQD28OM       246219280   \n",
       "2          US     52895410   R52R85WC6TIAH  B01489L5LQ       534732318   \n",
       "3          US     27072354   R7HOOYTVIB0DS  B008LOVIIK       239012694   \n",
       "4          US     26939022  R1XQ2N5CDOZGNX  B0094LZMT0       535858974   \n",
       "\n",
       "                           product_title        product_category  star_rating  \\\n",
       "0                  Enlightened: Season 1  Digital_Video_Download            5   \n",
       "1                                Vicious  Digital_Video_Download            5   \n",
       "2                            After Words  Digital_Video_Download            4   \n",
       "3  Masterpiece: Inspector Lewis Season 5  Digital_Video_Download            5   \n",
       "4                      On The Waterfront  Digital_Video_Download            5   \n",
       "\n",
       "   helpful_votes  total_votes vine verified_purchase  \\\n",
       "0              0            0    N                 Y   \n",
       "1              0            0    N                 Y   \n",
       "2             17           18    N                 Y   \n",
       "3              0            0    N                 Y   \n",
       "4              0            0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0         I loved it and I wish there was a season 3   \n",
       "1  As always it seems that the best shows come fr...   \n",
       "2                                     Charming movie   \n",
       "3                                         Five Stars   \n",
       "4               Brilliant film from beginning to end   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  I loved it and I wish there was a season 3... ...  2015-08-31  \n",
       "1  As always it seems that the best shows come fr...  2015-08-31  \n",
       "2  This movie isn't perfect, but it gets a lot of...  2015-08-31  \n",
       "3                excellant this is what tv should be  2015-08-31  \n",
       "4  Brilliant film from beginning to end. All of t...  2015-08-31  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz', delimiter='\\t',error_bad_lines=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this dataset includes information like:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written.\n",
    "\n",
    "For this example, let's limit ourselves to `customer_id`, `product_id`, and `star_rating`.  Including additional features in our recommendation system could be beneficial, but would require substantial processing (particularly the text data) which would take us beyond the scope of this notebook.\n",
    "\n",
    "*Note: we'll keep `product_title` on the dataset to help verify our recommendations later in the notebook, but it will not be used in algorithm training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['customer_id', 'product_id', 'star_rating', 'product_title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because most people haven't seen most videos, and people rate fewer videos than we actually watch, we'd expect our data to be sparse.  Our algorithm should work well with this sparse problem in general, but we may still want to clean out some of the long tail.  Let's look at some basic percentiles to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers\n",
      " 0.00       1.0\n",
      "0.01       1.0\n",
      "0.02       1.0\n",
      "0.03       1.0\n",
      "0.04       1.0\n",
      "0.05       1.0\n",
      "0.10       1.0\n",
      "0.25       1.0\n",
      "0.50       1.0\n",
      "0.75       2.0\n",
      "0.90       4.0\n",
      "0.95       5.0\n",
      "0.96       6.0\n",
      "0.97       7.0\n",
      "0.98       9.0\n",
      "0.99      13.0\n",
      "1.00    2704.0\n",
      "Name: customer_id, dtype: float64\n",
      "products\n",
      " 0.00        1.00\n",
      "0.01        1.00\n",
      "0.02        1.00\n",
      "0.03        1.00\n",
      "0.04        1.00\n",
      "0.05        1.00\n",
      "0.10        1.00\n",
      "0.25        1.00\n",
      "0.50        3.00\n",
      "0.75        9.00\n",
      "0.90       31.00\n",
      "0.95       73.00\n",
      "0.96       95.00\n",
      "0.97      130.00\n",
      "0.98      199.00\n",
      "0.99      386.67\n",
      "1.00    32790.00\n",
      "Name: product_id, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "customers = df['customer_id'].value_counts()\n",
    "products = df['product_id'].value_counts()\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "print('customers\\n', customers.quantile(quantiles))\n",
    "print('products\\n', products.quantile(quantiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only about 5% of customers have rated 5 or more videos, and only 25% of videos have been rated by 9+ customers.\n",
    "\n",
    "### Clean\n",
    "\n",
    "Let's filter out this long tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers[customers >= 5]\n",
    "products = products[products >= 10]\n",
    "\n",
    "reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll recreate our customer and product lists since there are customers with more than 5 reviews, but all of their reviews are on products with less than 5 reviews (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll number each user and item, giving them their own sequential index.  This will allow us to hold the information in a sparse format where the sequential indices indicate the row and column in our ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>product_title</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>5</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>10463</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>5</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>489</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44025160</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>5</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>32100</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18602179</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>5</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>2237</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14424972</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>5</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>32340</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  star_rating  \\\n",
       "0     27072354  B008LOVIIK            5   \n",
       "1     16030865  B008LOVIIK            5   \n",
       "2     44025160  B008LOVIIK            5   \n",
       "3     18602179  B008LOVIIK            5   \n",
       "4     14424972  B008LOVIIK            5   \n",
       "\n",
       "                           product_title   user  item  \n",
       "0  Masterpiece: Inspector Lewis Season 5  10463   107  \n",
       "1  Masterpiece: Inspector Lewis Season 5    489   107  \n",
       "2  Masterpiece: Inspector Lewis Season 5  32100   107  \n",
       "3  Masterpiece: Inspector Lewis Season 5   2237   107  \n",
       "4  Masterpiece: Inspector Lewis Season 5  32340   107  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0])})\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "\n",
    "Let's start by splitting in training and test sets.  This will allow us to estimate the model's accuracy on videos our customers rated, but wasn't included in our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reduced_df.groupby('customer_id').last().reset_index()\n",
    "\n",
    "train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n",
    "                            on=['customer_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can convert our Pandas DataFrames into MXNet NDArrays, use those to create a member of the SparseMatrixDataset class, and add that to an MXNet Data Iterator.  This process is the same for both test and control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train = gluon.data.ArrayDataset(nd.array(train_df['user'].values, dtype=np.float32),\n",
    "                                nd.array(train_df['item'].values, dtype=np.float32),\n",
    "                                nd.array(train_df['star_rating'].values, dtype=np.float32))\n",
    "test  = gluon.data.ArrayDataset(nd.array(test_df['user'].values, dtype=np.float32),\n",
    "                                nd.array(test_df['item'].values, dtype=np.float32),\n",
    "                                nd.array(test_df['star_rating'].values, dtype=np.float32))\n",
    "\n",
    "train_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\n",
    "test_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train Locally\n",
    "\n",
    "### Define Network\n",
    "\n",
    "Let's start by defining the neural network version of our matrix factorization task.  In this case, our network is quite simple.  The main components are:\n",
    "- [Embeddings](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Embedding) which turn our indexes into dense vectors of fixed size.  In this case, 64.\n",
    "- [Dense layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dense) with ReLU activation.  Each dense layer has the same number of units as our number of embeddings.  Our ReLU activation here also adds some non-linearity to our matrix factorization.\n",
    "- [Dropout layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dropout) which can be used to prevent over-fitting.\n",
    "- Matrix multiplication of our user matrix and our item matrix to create an estimate of our rating matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFBlock(gluon.HybridBlock):\n",
    "    def __init__(self, max_users, max_items, num_emb, dropout_p=0.5):\n",
    "        super(MFBlock, self).__init__()\n",
    "        \n",
    "        self.max_users = max_users\n",
    "        self.max_items = max_items\n",
    "        self.dropout_p = dropout_p\n",
    "        self.num_emb = num_emb\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.user_embeddings = gluon.nn.Embedding(max_users, num_emb)\n",
    "            self.item_embeddings = gluon.nn.Embedding(max_items, num_emb)\n",
    "            \n",
    "            self.dropout_user = gluon.nn.Dropout(dropout_p)\n",
    "            self.dropout_item = gluon.nn.Dropout(dropout_p)\n",
    "\n",
    "            self.dense_user   = gluon.nn.Dense(num_emb, activation='relu')\n",
    "            self.dense_item = gluon.nn.Dense(num_emb, activation='relu')\n",
    "            \n",
    "    def hybrid_forward(self, F, users, items):\n",
    "        a = self.user_embeddings(users)\n",
    "        a = self.dense_user(a)\n",
    "        \n",
    "        b = self.item_embeddings(items)\n",
    "        b = self.dense_item(b)\n",
    "\n",
    "        predictions = self.dropout_user(a) * self.dropout_item(b)     \n",
    "        predictions = F.sum(predictions, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 64\n",
    "\n",
    "net = MFBlock(max_users=customer_index.shape[0], \n",
    "              max_items=product_index.shape[0],\n",
    "              num_emb=num_embeddings,\n",
    "              dropout_p=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters\n",
    "\n",
    "Let's initialize network weights and set our optimization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.gpu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network parameters\n",
    "ctx = mx.gpu()\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=60),\n",
    "                                ctx=ctx,\n",
    "                                force_reinit=True)\n",
    "net.hybridize()\n",
    "\n",
    "# Set optimization parameters\n",
    "opt = 'sgd'\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "wd = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        opt,\n",
    "                        {'learning_rate': lr,\n",
    "                         'wd': wd,\n",
    "                         'momentum': momentum})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute\n",
    "\n",
    "Let's define a function to carry out the training of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(train_iter, test_iter, net, epochs, ctx):\n",
    "    \n",
    "    loss_function = gluon.loss.L2Loss()\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        print(\"epoch: {}\".format(e))\n",
    "        \n",
    "        for i, (user, item, label) in enumerate(train_iter):\n",
    "                user = user.as_in_context(ctx)\n",
    "                item = item.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                \n",
    "                with mx.autograd.record():\n",
    "                    output = net(user, item)               \n",
    "                    loss = loss_function(output, label)\n",
    "                    \n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "\n",
    "        print(\"EPOCH {}: MSE ON TRAINING and TEST: {}. {}\".format(e,\n",
    "                                                                   eval_net(train_iter, net, ctx, loss_function),\n",
    "                                                                   eval_net(test_iter, net, ctx, loss_function)))\n",
    "    print(\"end of training\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a function which evaluates our network on a given dataset.  This is called by our `execute` function above to provide mean squared error values on our training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(data, net, ctx, loss_function):\n",
    "    acc = MSE()\n",
    "    for i, (user, item, label) in enumerate(data):\n",
    "        \n",
    "            user = user.as_in_context(ctx)\n",
    "            item = item.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            predictions = net(user, item).reshape((batch_size, 1))\n",
    "            acc.update(preds=[predictions], labels=[label])\n",
    "   \n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "EPOCH 0: MSE ON TRAINING and TEST: 1.1478585849007763. 1.1478933943695282\n",
      "epoch: 1\n",
      "EPOCH 1: MSE ON TRAINING and TEST: 1.015684979429758. 1.0157101604330587\n",
      "epoch: 2\n",
      "EPOCH 2: MSE ON TRAINING and TEST: 0.958374810052583. 0.9584383050007607\n",
      "end of training\n",
      "CPU times: user 8min 55s, sys: 11.5 s, total: 9min 6s\n",
      "Wall time: 8min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "trained_net = execute(train_iter, test_iter, net, epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Validation\n",
    "\n",
    "We can see our training error going down, but our validation accuracy bounces around a bit.  Let's check how our model is predicting for an individual user.  We could pick randomly, but for this case, let's try user #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>product_id</th>\n",
       "      <th>u6_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>1505</td>\n",
       "      <td>B008ZXT1KO</td>\n",
       "      <td>5.651490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>641</td>\n",
       "      <td>B0088WLEVG</td>\n",
       "      <td>5.634015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>977</td>\n",
       "      <td>B006885EFE</td>\n",
       "      <td>5.575847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>3223</td>\n",
       "      <td>B00E9HE4M0</td>\n",
       "      <td>5.568130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>2856</td>\n",
       "      <td>B004LQGXPA</td>\n",
       "      <td>5.563286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>3600</td>\n",
       "      <td>B00YMIN7RE</td>\n",
       "      <td>5.552298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2965</th>\n",
       "      <td>2965</td>\n",
       "      <td>B00H8XP4W6</td>\n",
       "      <td>5.543756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19437</th>\n",
       "      <td>19437</td>\n",
       "      <td>B006RM32FA</td>\n",
       "      <td>5.518327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>656</td>\n",
       "      <td>B003X1BOIU</td>\n",
       "      <td>5.516575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>B00RT7FSQI</td>\n",
       "      <td>5.515037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17722</th>\n",
       "      <td>17722</td>\n",
       "      <td>B00E6M7VM8</td>\n",
       "      <td>5.514292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>10647</td>\n",
       "      <td>B004BZG1FI</td>\n",
       "      <td>5.513137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>1014</td>\n",
       "      <td>B003YV8IHE</td>\n",
       "      <td>5.512478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>213</td>\n",
       "      <td>B005LAN0AG</td>\n",
       "      <td>5.512251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13749</th>\n",
       "      <td>13749</td>\n",
       "      <td>B009FIMYBG</td>\n",
       "      <td>5.500501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>4366</td>\n",
       "      <td>B00UT3XFGE</td>\n",
       "      <td>5.496831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28365</th>\n",
       "      <td>28365</td>\n",
       "      <td>B00H1O476G</td>\n",
       "      <td>5.494884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>B007SPQZMC</td>\n",
       "      <td>5.494514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>924</td>\n",
       "      <td>B0011BB9XY</td>\n",
       "      <td>5.481512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28364</th>\n",
       "      <td>28364</td>\n",
       "      <td>B004W1GD72</td>\n",
       "      <td>5.480252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>2031</td>\n",
       "      <td>B00AH8SPOI</td>\n",
       "      <td>5.478979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5069</th>\n",
       "      <td>5069</td>\n",
       "      <td>B008ZXT2LM</td>\n",
       "      <td>5.478130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>2005</td>\n",
       "      <td>B005HG0H8W</td>\n",
       "      <td>5.473009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317</th>\n",
       "      <td>2317</td>\n",
       "      <td>B001XUH69Y</td>\n",
       "      <td>5.470793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>131</td>\n",
       "      <td>B00NL5R7NO</td>\n",
       "      <td>5.470622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>3947</td>\n",
       "      <td>B000IZZS5O</td>\n",
       "      <td>5.470615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>2154</td>\n",
       "      <td>B00V325O0U</td>\n",
       "      <td>5.456789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34087</th>\n",
       "      <td>34087</td>\n",
       "      <td>B00CB3EU0K</td>\n",
       "      <td>5.455829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>2785</td>\n",
       "      <td>B005HFLM3W</td>\n",
       "      <td>5.454263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8231</th>\n",
       "      <td>8231</td>\n",
       "      <td>B00BMFN8GK</td>\n",
       "      <td>5.453722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>1743</td>\n",
       "      <td>B00DBT6SI0</td>\n",
       "      <td>2.717396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6006</th>\n",
       "      <td>6006</td>\n",
       "      <td>B00A7YXXM6</td>\n",
       "      <td>2.716262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7064</th>\n",
       "      <td>7064</td>\n",
       "      <td>B000JNO77A</td>\n",
       "      <td>2.714577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5104</th>\n",
       "      <td>5104</td>\n",
       "      <td>B00RGHE2GI</td>\n",
       "      <td>2.691914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>1414</td>\n",
       "      <td>B00BLGXL3U</td>\n",
       "      <td>2.681516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>1621</td>\n",
       "      <td>B00KYKNSX2</td>\n",
       "      <td>2.659375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5483</th>\n",
       "      <td>5483</td>\n",
       "      <td>B00CU72CES</td>\n",
       "      <td>2.626019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>4079</td>\n",
       "      <td>B00SXCRLW2</td>\n",
       "      <td>2.609198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>4272</td>\n",
       "      <td>B00B99P718</td>\n",
       "      <td>2.606696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>7372</td>\n",
       "      <td>B00M5KODWO</td>\n",
       "      <td>2.598680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>B00JH3S0AI</td>\n",
       "      <td>2.591318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>5401</td>\n",
       "      <td>B00KAUDM3W</td>\n",
       "      <td>2.556758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>2236</td>\n",
       "      <td>B00DU6OIKE</td>\n",
       "      <td>2.537464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>3366</td>\n",
       "      <td>B00B99Q668</td>\n",
       "      <td>2.531118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4052</th>\n",
       "      <td>4052</td>\n",
       "      <td>B00DKII3GC</td>\n",
       "      <td>2.508129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>2875</td>\n",
       "      <td>B00RVC3F6Q</td>\n",
       "      <td>2.501589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>1296</td>\n",
       "      <td>B00MHTR3ZW</td>\n",
       "      <td>2.480220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6470</th>\n",
       "      <td>6470</td>\n",
       "      <td>B00BPRCKKA</td>\n",
       "      <td>2.441286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124</th>\n",
       "      <td>3124</td>\n",
       "      <td>B006GLM52I</td>\n",
       "      <td>2.412714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>2774</td>\n",
       "      <td>B00CU7NVYS</td>\n",
       "      <td>2.398010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>537</td>\n",
       "      <td>B00KG2Q8US</td>\n",
       "      <td>2.375031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>2922</td>\n",
       "      <td>B002DTJ03E</td>\n",
       "      <td>2.369883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>1983</td>\n",
       "      <td>B00DD2BK2O</td>\n",
       "      <td>2.364244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967</th>\n",
       "      <td>3967</td>\n",
       "      <td>B00NESTCC2</td>\n",
       "      <td>2.361089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2826</th>\n",
       "      <td>2826</td>\n",
       "      <td>B00A7YXU0G</td>\n",
       "      <td>2.352441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>2508</td>\n",
       "      <td>B00LBVQW3Q</td>\n",
       "      <td>2.299764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>782</td>\n",
       "      <td>B00M90R2D2</td>\n",
       "      <td>2.272156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>707</td>\n",
       "      <td>B00DD2B52Y</td>\n",
       "      <td>2.251050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4628</th>\n",
       "      <td>4628</td>\n",
       "      <td>B00DKGWLGM</td>\n",
       "      <td>2.236820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>3651</td>\n",
       "      <td>B00M5P22D6</td>\n",
       "      <td>2.050800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38385 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item  product_id  u6_predictions\n",
       "1505    1505  B008ZXT1KO        5.651490\n",
       "641      641  B0088WLEVG        5.634015\n",
       "977      977  B006885EFE        5.575847\n",
       "3223    3223  B00E9HE4M0        5.568130\n",
       "2856    2856  B004LQGXPA        5.563286\n",
       "3600    3600  B00YMIN7RE        5.552298\n",
       "2965    2965  B00H8XP4W6        5.543756\n",
       "19437  19437  B006RM32FA        5.518327\n",
       "656      656  B003X1BOIU        5.516575\n",
       "172      172  B00RT7FSQI        5.515037\n",
       "17722  17722  B00E6M7VM8        5.514292\n",
       "10647  10647  B004BZG1FI        5.513137\n",
       "1014    1014  B003YV8IHE        5.512478\n",
       "213      213  B005LAN0AG        5.512251\n",
       "13749  13749  B009FIMYBG        5.500501\n",
       "4366    4366  B00UT3XFGE        5.496831\n",
       "28365  28365  B00H1O476G        5.494884\n",
       "2          2  B007SPQZMC        5.494514\n",
       "924      924  B0011BB9XY        5.481512\n",
       "28364  28364  B004W1GD72        5.480252\n",
       "2031    2031  B00AH8SPOI        5.478979\n",
       "5069    5069  B008ZXT2LM        5.478130\n",
       "2005    2005  B005HG0H8W        5.473009\n",
       "2317    2317  B001XUH69Y        5.470793\n",
       "131      131  B00NL5R7NO        5.470622\n",
       "3947    3947  B000IZZS5O        5.470615\n",
       "2154    2154  B00V325O0U        5.456789\n",
       "34087  34087  B00CB3EU0K        5.455829\n",
       "2785    2785  B005HFLM3W        5.454263\n",
       "8231    8231  B00BMFN8GK        5.453722\n",
       "...      ...         ...             ...\n",
       "1743    1743  B00DBT6SI0        2.717396\n",
       "6006    6006  B00A7YXXM6        2.716262\n",
       "7064    7064  B000JNO77A        2.714577\n",
       "5104    5104  B00RGHE2GI        2.691914\n",
       "1414    1414  B00BLGXL3U        2.681516\n",
       "1621    1621  B00KYKNSX2        2.659375\n",
       "5483    5483  B00CU72CES        2.626019\n",
       "4079    4079  B00SXCRLW2        2.609198\n",
       "4272    4272  B00B99P718        2.606696\n",
       "7372    7372  B00M5KODWO        2.598680\n",
       "196      196  B00JH3S0AI        2.591318\n",
       "5401    5401  B00KAUDM3W        2.556758\n",
       "2236    2236  B00DU6OIKE        2.537464\n",
       "3366    3366  B00B99Q668        2.531118\n",
       "4052    4052  B00DKII3GC        2.508129\n",
       "2875    2875  B00RVC3F6Q        2.501589\n",
       "1296    1296  B00MHTR3ZW        2.480220\n",
       "6470    6470  B00BPRCKKA        2.441286\n",
       "3124    3124  B006GLM52I        2.412714\n",
       "2774    2774  B00CU7NVYS        2.398010\n",
       "537      537  B00KG2Q8US        2.375031\n",
       "2922    2922  B002DTJ03E        2.369883\n",
       "1983    1983  B00DD2BK2O        2.364244\n",
       "3967    3967  B00NESTCC2        2.361089\n",
       "2826    2826  B00A7YXU0G        2.352441\n",
       "2508    2508  B00LBVQW3Q        2.299764\n",
       "782      782  B00M90R2D2        2.272156\n",
       "707      707  B00DD2B52Y        2.251050\n",
       "4628    4628  B00DKGWLGM        2.236820\n",
       "3651    3651  B00M5P22D6        2.050800\n",
       "\n",
       "[38385 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_index['u6_predictions'] = trained_net(nd.array([6] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u6_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this to the predictions for another user (we'll try user #7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>product_id</th>\n",
       "      <th>u6_predictions</th>\n",
       "      <th>u7_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>1505</td>\n",
       "      <td>B008ZXT1KO</td>\n",
       "      <td>5.651490</td>\n",
       "      <td>4.007372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>10647</td>\n",
       "      <td>B004BZG1FI</td>\n",
       "      <td>5.513137</td>\n",
       "      <td>4.003162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>1014</td>\n",
       "      <td>B003YV8IHE</td>\n",
       "      <td>5.512478</td>\n",
       "      <td>3.984093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>1291</td>\n",
       "      <td>B00L0Y5Y8I</td>\n",
       "      <td>5.425984</td>\n",
       "      <td>3.982222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28364</th>\n",
       "      <td>28364</td>\n",
       "      <td>B004W1GD72</td>\n",
       "      <td>5.480252</td>\n",
       "      <td>3.981879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>656</td>\n",
       "      <td>B003X1BOIU</td>\n",
       "      <td>5.516575</td>\n",
       "      <td>3.977365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>3600</td>\n",
       "      <td>B00YMIN7RE</td>\n",
       "      <td>5.552298</td>\n",
       "      <td>3.976738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>2856</td>\n",
       "      <td>B004LQGXPA</td>\n",
       "      <td>5.563286</td>\n",
       "      <td>3.971005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34754</th>\n",
       "      <td>34754</td>\n",
       "      <td>B00VXW8NI0</td>\n",
       "      <td>5.428771</td>\n",
       "      <td>3.970542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>6090</td>\n",
       "      <td>B001GQLSVW</td>\n",
       "      <td>5.334682</td>\n",
       "      <td>3.963368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17722</th>\n",
       "      <td>17722</td>\n",
       "      <td>B00E6M7VM8</td>\n",
       "      <td>5.514292</td>\n",
       "      <td>3.956327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>B00RT7FSQI</td>\n",
       "      <td>5.515037</td>\n",
       "      <td>3.947726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>213</td>\n",
       "      <td>B005LAN0AG</td>\n",
       "      <td>5.512251</td>\n",
       "      <td>3.947614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>1520</td>\n",
       "      <td>B005HFLNDQ</td>\n",
       "      <td>5.386960</td>\n",
       "      <td>3.946412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>131</td>\n",
       "      <td>B00NL5R7NO</td>\n",
       "      <td>5.470622</td>\n",
       "      <td>3.942998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>669</td>\n",
       "      <td>B0011BDPWW</td>\n",
       "      <td>5.395681</td>\n",
       "      <td>3.942737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>2968</td>\n",
       "      <td>B00L0Y7JAY</td>\n",
       "      <td>5.425273</td>\n",
       "      <td>3.938132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>641</td>\n",
       "      <td>B0088WLEVG</td>\n",
       "      <td>5.634015</td>\n",
       "      <td>3.933080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>2916</td>\n",
       "      <td>B0036D8M9E</td>\n",
       "      <td>5.433945</td>\n",
       "      <td>3.932610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>924</td>\n",
       "      <td>B0011BB9XY</td>\n",
       "      <td>5.481512</td>\n",
       "      <td>3.932581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28365</th>\n",
       "      <td>28365</td>\n",
       "      <td>B00H1O476G</td>\n",
       "      <td>5.494884</td>\n",
       "      <td>3.930635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>2308</td>\n",
       "      <td>B00QLF2YLG</td>\n",
       "      <td>5.416163</td>\n",
       "      <td>3.928599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>1828</td>\n",
       "      <td>B003DJ2JC2</td>\n",
       "      <td>5.372982</td>\n",
       "      <td>3.926444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15829</th>\n",
       "      <td>15829</td>\n",
       "      <td>B00AALUFFY</td>\n",
       "      <td>5.339453</td>\n",
       "      <td>3.926262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>2884</td>\n",
       "      <td>B0049J0DC8</td>\n",
       "      <td>5.395122</td>\n",
       "      <td>3.925519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>5.451094</td>\n",
       "      <td>3.924830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317</th>\n",
       "      <td>2317</td>\n",
       "      <td>B001XUH69Y</td>\n",
       "      <td>5.470793</td>\n",
       "      <td>3.923681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>B007SPQZMC</td>\n",
       "      <td>5.494514</td>\n",
       "      <td>3.916656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>3655</td>\n",
       "      <td>B003925164</td>\n",
       "      <td>5.381374</td>\n",
       "      <td>3.915906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>1526</td>\n",
       "      <td>B005HQKXIG</td>\n",
       "      <td>5.384189</td>\n",
       "      <td>3.914014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>2814</td>\n",
       "      <td>B00HNMNOD8</td>\n",
       "      <td>2.815147</td>\n",
       "      <td>1.901549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>1201</td>\n",
       "      <td>B00IKP5XNQ</td>\n",
       "      <td>2.732575</td>\n",
       "      <td>1.888745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6006</th>\n",
       "      <td>6006</td>\n",
       "      <td>B00A7YXXM6</td>\n",
       "      <td>2.716262</td>\n",
       "      <td>1.878794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>5401</td>\n",
       "      <td>B00KAUDM3W</td>\n",
       "      <td>2.556758</td>\n",
       "      <td>1.862742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>7372</td>\n",
       "      <td>B00M5KODWO</td>\n",
       "      <td>2.598680</td>\n",
       "      <td>1.860889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>4272</td>\n",
       "      <td>B00B99P718</td>\n",
       "      <td>2.606696</td>\n",
       "      <td>1.860391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>B00JH3S0AI</td>\n",
       "      <td>2.591318</td>\n",
       "      <td>1.860021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>1562</td>\n",
       "      <td>B00KG2QLCI</td>\n",
       "      <td>2.764635</td>\n",
       "      <td>1.857638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>4633</td>\n",
       "      <td>B00OHCJ3UU</td>\n",
       "      <td>2.734313</td>\n",
       "      <td>1.848845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>4079</td>\n",
       "      <td>B00SXCRLW2</td>\n",
       "      <td>2.609198</td>\n",
       "      <td>1.825250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>1012</td>\n",
       "      <td>B00LTMIAF0</td>\n",
       "      <td>2.737586</td>\n",
       "      <td>1.817792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>1296</td>\n",
       "      <td>B00MHTR3ZW</td>\n",
       "      <td>2.480220</td>\n",
       "      <td>1.799452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124</th>\n",
       "      <td>3124</td>\n",
       "      <td>B006GLM52I</td>\n",
       "      <td>2.412714</td>\n",
       "      <td>1.767569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5483</th>\n",
       "      <td>5483</td>\n",
       "      <td>B00CU72CES</td>\n",
       "      <td>2.626019</td>\n",
       "      <td>1.758465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>2875</td>\n",
       "      <td>B00RVC3F6Q</td>\n",
       "      <td>2.501589</td>\n",
       "      <td>1.739723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>2236</td>\n",
       "      <td>B00DU6OIKE</td>\n",
       "      <td>2.537464</td>\n",
       "      <td>1.731832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>3366</td>\n",
       "      <td>B00B99Q668</td>\n",
       "      <td>2.531118</td>\n",
       "      <td>1.705749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>2922</td>\n",
       "      <td>B002DTJ03E</td>\n",
       "      <td>2.369883</td>\n",
       "      <td>1.701210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6470</th>\n",
       "      <td>6470</td>\n",
       "      <td>B00BPRCKKA</td>\n",
       "      <td>2.441286</td>\n",
       "      <td>1.695981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>2508</td>\n",
       "      <td>B00LBVQW3Q</td>\n",
       "      <td>2.299764</td>\n",
       "      <td>1.687347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967</th>\n",
       "      <td>3967</td>\n",
       "      <td>B00NESTCC2</td>\n",
       "      <td>2.361089</td>\n",
       "      <td>1.663315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>1983</td>\n",
       "      <td>B00DD2BK2O</td>\n",
       "      <td>2.364244</td>\n",
       "      <td>1.661308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4052</th>\n",
       "      <td>4052</td>\n",
       "      <td>B00DKII3GC</td>\n",
       "      <td>2.508129</td>\n",
       "      <td>1.654585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>537</td>\n",
       "      <td>B00KG2Q8US</td>\n",
       "      <td>2.375031</td>\n",
       "      <td>1.639384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2826</th>\n",
       "      <td>2826</td>\n",
       "      <td>B00A7YXU0G</td>\n",
       "      <td>2.352441</td>\n",
       "      <td>1.637136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>782</td>\n",
       "      <td>B00M90R2D2</td>\n",
       "      <td>2.272156</td>\n",
       "      <td>1.595985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4628</th>\n",
       "      <td>4628</td>\n",
       "      <td>B00DKGWLGM</td>\n",
       "      <td>2.236820</td>\n",
       "      <td>1.591497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>2774</td>\n",
       "      <td>B00CU7NVYS</td>\n",
       "      <td>2.398010</td>\n",
       "      <td>1.580653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>707</td>\n",
       "      <td>B00DD2B52Y</td>\n",
       "      <td>2.251050</td>\n",
       "      <td>1.515700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>3651</td>\n",
       "      <td>B00M5P22D6</td>\n",
       "      <td>2.050800</td>\n",
       "      <td>1.481585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38385 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item  product_id  u6_predictions  u7_predictions\n",
       "1505    1505  B008ZXT1KO        5.651490        4.007372\n",
       "10647  10647  B004BZG1FI        5.513137        4.003162\n",
       "1014    1014  B003YV8IHE        5.512478        3.984093\n",
       "1291    1291  B00L0Y5Y8I        5.425984        3.982222\n",
       "28364  28364  B004W1GD72        5.480252        3.981879\n",
       "656      656  B003X1BOIU        5.516575        3.977365\n",
       "3600    3600  B00YMIN7RE        5.552298        3.976738\n",
       "2856    2856  B004LQGXPA        5.563286        3.971005\n",
       "34754  34754  B00VXW8NI0        5.428771        3.970542\n",
       "6090    6090  B001GQLSVW        5.334682        3.963368\n",
       "17722  17722  B00E6M7VM8        5.514292        3.956327\n",
       "172      172  B00RT7FSQI        5.515037        3.947726\n",
       "213      213  B005LAN0AG        5.512251        3.947614\n",
       "1520    1520  B005HFLNDQ        5.386960        3.946412\n",
       "131      131  B00NL5R7NO        5.470622        3.942998\n",
       "669      669  B0011BDPWW        5.395681        3.942737\n",
       "2968    2968  B00L0Y7JAY        5.425273        3.938132\n",
       "641      641  B0088WLEVG        5.634015        3.933080\n",
       "2916    2916  B0036D8M9E        5.433945        3.932610\n",
       "924      924  B0011BB9XY        5.481512        3.932581\n",
       "28365  28365  B00H1O476G        5.494884        3.930635\n",
       "2308    2308  B00QLF2YLG        5.416163        3.928599\n",
       "1828    1828  B003DJ2JC2        5.372982        3.926444\n",
       "15829  15829  B00AALUFFY        5.339453        3.926262\n",
       "2884    2884  B0049J0DC8        5.395122        3.925519\n",
       "106      106  B008LOVIIK        5.451094        3.924830\n",
       "2317    2317  B001XUH69Y        5.470793        3.923681\n",
       "2          2  B007SPQZMC        5.494514        3.916656\n",
       "3655    3655  B003925164        5.381374        3.915906\n",
       "1526    1526  B005HQKXIG        5.384189        3.914014\n",
       "...      ...         ...             ...             ...\n",
       "2814    2814  B00HNMNOD8        2.815147        1.901549\n",
       "1201    1201  B00IKP5XNQ        2.732575        1.888745\n",
       "6006    6006  B00A7YXXM6        2.716262        1.878794\n",
       "5401    5401  B00KAUDM3W        2.556758        1.862742\n",
       "7372    7372  B00M5KODWO        2.598680        1.860889\n",
       "4272    4272  B00B99P718        2.606696        1.860391\n",
       "196      196  B00JH3S0AI        2.591318        1.860021\n",
       "1562    1562  B00KG2QLCI        2.764635        1.857638\n",
       "4633    4633  B00OHCJ3UU        2.734313        1.848845\n",
       "4079    4079  B00SXCRLW2        2.609198        1.825250\n",
       "1012    1012  B00LTMIAF0        2.737586        1.817792\n",
       "1296    1296  B00MHTR3ZW        2.480220        1.799452\n",
       "3124    3124  B006GLM52I        2.412714        1.767569\n",
       "5483    5483  B00CU72CES        2.626019        1.758465\n",
       "2875    2875  B00RVC3F6Q        2.501589        1.739723\n",
       "2236    2236  B00DU6OIKE        2.537464        1.731832\n",
       "3366    3366  B00B99Q668        2.531118        1.705749\n",
       "2922    2922  B002DTJ03E        2.369883        1.701210\n",
       "6470    6470  B00BPRCKKA        2.441286        1.695981\n",
       "2508    2508  B00LBVQW3Q        2.299764        1.687347\n",
       "3967    3967  B00NESTCC2        2.361089        1.663315\n",
       "1983    1983  B00DD2BK2O        2.364244        1.661308\n",
       "4052    4052  B00DKII3GC        2.508129        1.654585\n",
       "537      537  B00KG2Q8US        2.375031        1.639384\n",
       "2826    2826  B00A7YXU0G        2.352441        1.637136\n",
       "782      782  B00M90R2D2        2.272156        1.595985\n",
       "4628    4628  B00DKGWLGM        2.236820        1.591497\n",
       "2774    2774  B00CU7NVYS        2.398010        1.580653\n",
       "707      707  B00DD2B52Y        2.251050        1.515700\n",
       "3651    3651  B00M5P22D6        2.050800        1.481585\n",
       "\n",
       "[38385 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_index['u7_predictions'] = trained_net(nd.array([7] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u7_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted ratings are different between the two users, but the same top (and bottom) items for user #6 appear for #7 as well.  Let's look at the correlation across the full set of 38K items to see if this relationship holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "product_index[['u6_predictions', 'u7_predictions']].plot.scatter('u6_predictions', 'u7_predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this correlation is nearly perfect.  Essentially the average rating of items dominates across users and we'll recommend the same well-reviewed items to everyone.  As it turns out, we can add more embeddings and this relationship will go away since we're better able to capture differential preferences across users.\n",
    "\n",
    "However, with just a 64 dimensional embedding, it took 7 minutes to run just 3 epochs.  If we ran this outside of our Notebook Instance we could run larger jobs and move on to other work would improve productivity.\n",
    "\n",
    "---\n",
    "\n",
    "## Train with SageMaker\n",
    "\n",
    "Now that we've trained on this smaller dataset, we can expand training in SageMaker's distributed, managed training environment.\n",
    "\n",
    "### Wrap Code\n",
    "\n",
    "To use SageMaker's pre-built MXNet container, we'll need to wrap our code from above into a Python script.  There's a great deal of flexibility in using SageMaker's pre-built containers, and detailed documentation can be found [here](https://github.com/aws/sagemaker-python-sdk#mxnet-sagemaker-estimators), but for our example, it consisted of:\n",
    "1. Wrapping all data preparation into a `prepare_train_data` function (we could name this whatever we like)\n",
    "1. Copying and pasting classes and functions from above word-for-word\n",
    "1. Defining a `train` function that:\n",
    "  1. Adds a bit of new code to pick up the input TSV dataset on the SageMaker Training cluster\n",
    "  1. Takes in a dict of hyperparameters (which we specified as globals above)\n",
    "  1. Creates the net and executes training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import logging\r\n",
      "import json\r\n",
      "import time\r\n",
      "import os\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, nd, ndarray\r\n",
      "from mxnet.metric import MSE\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "os.system('pip install pandas')\r\n",
      "import pandas as pd\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "#########\r\n",
      "# Globals\r\n",
      "#########\r\n",
      "\r\n",
      "batch_size = 1024\r\n",
      "\r\n",
      "\r\n",
      "##########\r\n",
      "# Training\r\n",
      "##########\r\n",
      "\r\n",
      "def train(channel_input_dirs, hyperparameters, hosts, num_gpus, **kwargs):\r\n",
      "    \r\n",
      "    # get data\r\n",
      "    training_dir = channel_input_dirs['train']\r\n",
      "    train_iter, test_iter, customer_index, product_index = prepare_train_data(training_dir)\r\n",
      "    \r\n",
      "    # get hyperparameters\r\n",
      "    num_embeddings = hyperparameters.get('num_embeddings', 64)\r\n",
      "    opt = hyperparameters.get('opt', 'sgd')\r\n",
      "    lr = hyperparameters.get('lr', 0.02)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    wd = hyperparameters.get('wd', 0.)\r\n",
      "    epochs = hyperparameters.get('epochs', 5)\r\n",
      "\r\n",
      "    # define net\r\n",
      "    ctx = mx.gpu()\r\n",
      "\r\n",
      "    net = MFBlock(max_users=customer_index.shape[0], \r\n",
      "                  max_items=product_index.shape[0],\r\n",
      "                  num_emb=num_embeddings,\r\n",
      "                  dropout_p=0.5)\r\n",
      "    \r\n",
      "    net.collect_params().initialize(mx.init.Xavier(magnitude=60),\r\n",
      "                                    ctx=ctx,\r\n",
      "                                    force_reinit=True)\r\n",
      "    net.hybridize()\r\n",
      "\r\n",
      "    trainer = gluon.Trainer(net.collect_params(),\r\n",
      "                            opt,\r\n",
      "                            {'learning_rate': lr,\r\n",
      "                             'wd': wd,\r\n",
      "                             'momentum': momentum})\r\n",
      "    \r\n",
      "    # execute\r\n",
      "    trained_net = execute(train_iter, test_iter, net, trainer, epochs, ctx)\r\n",
      "    \r\n",
      "    return trained_net, customer_index, product_index\r\n",
      "\r\n",
      "\r\n",
      "class MFBlock(gluon.HybridBlock):\r\n",
      "    def __init__(self, max_users, max_items, num_emb, dropout_p=0.5):\r\n",
      "        super(MFBlock, self).__init__()\r\n",
      "        \r\n",
      "        self.max_users = max_users\r\n",
      "        self.max_items = max_items\r\n",
      "        self.dropout_p = dropout_p\r\n",
      "        self.num_emb = num_emb\r\n",
      "        \r\n",
      "        with self.name_scope():\r\n",
      "            self.user_embeddings = gluon.nn.Embedding(max_users, num_emb)\r\n",
      "            self.item_embeddings = gluon.nn.Embedding(max_items, num_emb)\r\n",
      "\r\n",
      "            self.dropout_user = gluon.nn.Dropout(dropout_p)\r\n",
      "            self.dropout_item = gluon.nn.Dropout(dropout_p)\r\n",
      "\r\n",
      "            self.dense_user   = gluon.nn.Dense(num_emb, activation='relu')\r\n",
      "            self.dense_item = gluon.nn.Dense(num_emb, activation='relu')\r\n",
      "            \r\n",
      "    def hybrid_forward(self, F, users, items):\r\n",
      "        a = self.user_embeddings(users)\r\n",
      "        a = self.dense_user(a)\r\n",
      "        \r\n",
      "        b = self.item_embeddings(items)\r\n",
      "        b = self.dense_item(b)\r\n",
      "\r\n",
      "        predictions = self.dropout_user(a) * self.dropout_item(b)      \r\n",
      "        predictions = F.sum(predictions, axis=1)\r\n",
      "\r\n",
      "        return predictions\r\n",
      "\r\n",
      "    \r\n",
      "def execute(train_iter, test_iter, net, trainer, epochs, ctx):\r\n",
      "    loss_function = gluon.loss.L2Loss()\r\n",
      "    for e in range(epochs):\r\n",
      "        print(\"epoch: {}\".format(e))\r\n",
      "        for i, (user, item, label) in enumerate(train_iter):\r\n",
      "\r\n",
      "                user = user.as_in_context(ctx)\r\n",
      "                item = item.as_in_context(ctx)\r\n",
      "                label = label.as_in_context(ctx)\r\n",
      "\r\n",
      "                with mx.autograd.record():\r\n",
      "                    output = net(user, item)               \r\n",
      "                    loss = loss_function(output, label)\r\n",
      "                loss.backward()\r\n",
      "                trainer.step(batch_size)\r\n",
      "\r\n",
      "        print(\"EPOCH {}: MSE ON TRAINING and TEST: {}. {}\".format(e,\r\n",
      "                                                                   eval_net(train_iter, net, ctx, loss_function),\r\n",
      "                                                                   eval_net(test_iter, net, ctx, loss_function)))\r\n",
      "    print(\"end of training\")\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def eval_net(data, net, ctx, loss_function):\r\n",
      "    acc = MSE()\r\n",
      "    for i, (user, item, label) in enumerate(data):\r\n",
      "\r\n",
      "            user = user.as_in_context(ctx)\r\n",
      "            item = item.as_in_context(ctx)\r\n",
      "            label = label.as_in_context(ctx)\r\n",
      "\r\n",
      "            predictions = net(user, item).reshape((batch_size, 1))\r\n",
      "            acc.update(preds=[predictions], labels=[label])\r\n",
      "\r\n",
      "    return acc.get()[1]\r\n",
      "\r\n",
      "\r\n",
      "def save(model, model_dir):\r\n",
      "    net, customer_index, product_index = model\r\n",
      "    net.save_params('{}/model.params'.format(model_dir))\r\n",
      "    f = open('{}/MFBlock.params'.format(model_dir), 'w')\r\n",
      "    json.dump({'max_users': net.max_users,\r\n",
      "               'max_items': net.max_items,\r\n",
      "               'num_emb': net.num_emb,\r\n",
      "               'dropout_p': net.dropout_p},\r\n",
      "              f)\r\n",
      "    f.close()\r\n",
      "    customer_index.to_csv('{}/customer_index.csv'.format(model_dir), index=False)\r\n",
      "    product_index.to_csv('{}/product_index.csv'.format(model_dir), index=False)\r\n",
      "\r\n",
      "    \r\n",
      "######\r\n",
      "# Data\r\n",
      "######\r\n",
      "\r\n",
      "def prepare_train_data(training_dir):\r\n",
      "    f = os.listdir(training_dir)\r\n",
      "    df = pd.read_csv(os.path.join(training_dir, f[0]), delimiter='\\t', error_bad_lines=False)\r\n",
      "    df = df[['customer_id', 'product_id', 'star_rating']]\r\n",
      "    customers = df['customer_id'].value_counts()\r\n",
      "    products = df['product_id'].value_counts()\r\n",
      "    \r\n",
      "    # Filter long-tail\r\n",
      "    customers = customers[customers >= 5]\r\n",
      "    products = products[products >= 10]\r\n",
      "\r\n",
      "    reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))\r\n",
      "    customers = reduced_df['customer_id'].value_counts()\r\n",
      "    products = reduced_df['product_id'].value_counts()\r\n",
      "\r\n",
      "    # Number users and items\r\n",
      "    customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\r\n",
      "    product_index = pd.DataFrame({'product_id': products.index, 'item': np.arange(products.shape[0])})\r\n",
      "\r\n",
      "    reduced_df = reduced_df.merge(customer_index).merge(product_index)\r\n",
      "\r\n",
      "    # Split train and test\r\n",
      "    test_df = reduced_df.groupby('customer_id').last().reset_index()\r\n",
      "\r\n",
      "    train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \r\n",
      "                                on=['customer_id', 'product_id'], \r\n",
      "                                how='outer', \r\n",
      "                                indicator=True)\r\n",
      "    train_df = train_df[(train_df['_merge'] == 'left_only')]\r\n",
      "\r\n",
      "    # MXNet data iterators\r\n",
      "    train = gluon.data.ArrayDataset(nd.array(train_df['user'].values, dtype=np.float32), \r\n",
      "                                    nd.array(train_df['item'].values, dtype=np.float32),\r\n",
      "                                    nd.array(train_df['star_rating'].values, dtype=np.float32))\r\n",
      "    test  = gluon.data.ArrayDataset(nd.array(test_df['user'].values, dtype=np.float32), \r\n",
      "                                    nd.array(test_df['item'].values, dtype=np.float32),\r\n",
      "                                    nd.array(test_df['star_rating'].values, dtype=np.float32))\r\n",
      "\r\n",
      "    train_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\r\n",
      "    test_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\r\n",
      "\r\n",
      "    return train_iter, test_iter, customer_index, product_index \r\n",
      "\r\n",
      "#########\r\n",
      "# Hosting\r\n",
      "#########\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    Load the gluon model. Called once when hosting service starts.\r\n",
      "\r\n",
      "    :param: model_dir The directory where model files are stored.\r\n",
      "    :return: a model (in this case a Gluon network)\r\n",
      "    \"\"\"\r\n",
      "    ctx = mx.cpu()\r\n",
      "    f = open('{}/MFBlock.params'.format(model_dir), 'r')\r\n",
      "    block_params = json.load(f)\r\n",
      "    f.close()\r\n",
      "    net = MFBlock(max_users=block_params['max_users'], \r\n",
      "                  max_items=block_params['max_items'],\r\n",
      "                  num_emb=block_params['num_emb'],\r\n",
      "                  dropout_p=block_params['dropout_p'])\r\n",
      "    net.load_params('{}/model.params'.format(model_dir), ctx)\r\n",
      "    customer_index = pd.read_csv('{}/customer_index.csv'.format(model_dir))\r\n",
      "    product_index = pd.read_csv('{}/product_index.csv'.format(model_dir))\r\n",
      "    return net, customer_index, product_index\r\n",
      "\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    \"\"\"\r\n",
      "    Transform a request using the Gluon model. Called once per request.\r\n",
      "\r\n",
      "    :param net: The Gluon model.\r\n",
      "    :param data: The request payload.\r\n",
      "    :param input_content_type: The request content type.\r\n",
      "    :param output_content_type: The (desired) response content type.\r\n",
      "    :return: response payload and content type.\r\n",
      "    \"\"\"\r\n",
      "    ctx = mx.cpu()\r\n",
      "    parsed = json.loads(data)\r\n",
      "\r\n",
      "    trained_net, customer_index, product_index = net\r\n",
      "    users = pd.DataFrame({'customer_id': parsed['customer_id']}).merge(customer_index, how='left')['user'].values\r\n",
      "    items = pd.DataFrame({'product_id': parsed['product_id']}).merge(product_index, how='left')['item'].values\r\n",
      "    \r\n",
      "    predictions = trained_net(nd.array(users).as_in_context(ctx), nd.array(items).as_in_context(ctx))\r\n",
      "    response_body = json.dumps(predictions.asnumpy().tolist())\r\n",
      "\r\n",
      "    return response_body, output_content_type\r\n"
     ]
    }
   ],
   "source": [
    "!cat recommender.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Now we can test our train function locally.  This helps ensure we don't have any bugs before submitting our code to SageMaker's pre-built MXNet container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 92523: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 343254: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 524626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 623024: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 977412: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1496867: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1711638: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1787213: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2395306: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2527690: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "EPOCH 0: MSE ON TRAINING and TEST: 1.1765037772664986. 1.1764914449230133\n",
      "epoch: 1\n",
      "EPOCH 1: MSE ON TRAINING and TEST: 1.034049305426647. 1.0340361617713336\n",
      "epoch: 2\n",
      "EPOCH 2: MSE ON TRAINING and TEST: 0.9965512728786089. 0.9965402909179232\n",
      "end of training\n",
      "CPU times: user 10min 14s, sys: 13.3 s, total: 10min 27s\n",
      "Wall time: 9min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import recommender\n",
    "\n",
    "local_test_net, local_customer_index, local_product_index = recommender.train(\n",
    "    {'train': '/tmp/recsys/'}, \n",
    "    {'num_embeddings': 64, \n",
    "     'opt': 'sgd', \n",
    "     'lr': 0.02, \n",
    "     'momentum': 0.9, \n",
    "     'wd': 0.,\n",
    "     'epochs': 3},\n",
    "    ['local'],\n",
    "    1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wrap-up\n",
    "\n",
    "In this example, we developed a deep learning model to predict customer ratings.  This could serve as the foundation of a recommender system in a variety of use cases.  However, there are many ways in which it could be improved.  For example we did very little with:\n",
    "- hyperparameter tuning\n",
    "- controlling for overfitting (early stopping, dropout, etc.)\n",
    "- testing whether binarizing our target variable would improve results\n",
    "- including other information sources (video genres, historical ratings, time of review)\n",
    "- adjusting our threshold for user and item inclusion \n",
    "\n",
    "In addition to improving the model, we could improve the engineering by:\n",
    "- Setting the context and key value store up for distributed training\n",
    "- Fine tuning our data ingestion (e.g. num_workers on our data iterators) to ensure we're fully utilizing our GPU\n",
    "- Thinking about how pre-processing would need to change as datasets scale beyond a single machine\n",
    "\n",
    "Beyond that, recommenders are a very active area of research and techniques from active learning, reinforcement learning, segmentation, ensembling, and more should be investigated to deliver well-rounded recommendations.\n",
    "\n",
    "### Clean-up (optional)\n",
    "\n",
    "Let's finish by deleting our endpoint to avoid stray hosting charges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
